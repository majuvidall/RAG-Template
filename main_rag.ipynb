{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Template de RAG Experimental**\n",
    "\n",
    "> *Pipeline modular de Recuperação e Geração com Langchain + OpenAI*\n",
    "\n",
    "**`Data:`** **Julho de 2025**\n",
    "\n",
    "**`Objetivo:`** Este notebook tem como objetivo servir como **template base para construção de pipelines RAG (Retrieval-Augmented Generation)** usando Langchain. \n",
    "\n",
    "**`Autoria:`** Desenvolvido pelo  **Núcleo de Inteligência Artificial do PD&I** da Radix. \n",
    "\n",
    "**`Equipe Técnica:`**\n",
    "\n",
    "|<img src=\"https://i.imgur.com/HvkmiXf.png\" width=\"100\"> | <img src=\"https://i.imgur.com/27pmxnN.jpeg\" width=\"100\" > | <img src=\"https://i.imgur.com/enG4rPa.jpeg\" width=\"100\"> | <img src=\"https://i.imgur.com/FSL0u7D.jpeg\" width=\"100\" > |\n",
    "|:--:|:--:|:--:|:--:|\n",
    "| Maria Júlia Vidal | Éric Kauati | Milena Toledo | Fabio Contrera |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RAG (Retrieval-Augmented Generation)** \n",
    "##### **RAG** é sigla para *Retrieval Augmented Generation*, uma técnica que amplia a capacidade de resposta de modelos de linguagem (LLMs) ao combinar seu conhecimento interno com informações recuperadas de fontes externas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O funcionamento da RAG pode ser entendido como uma colaboração entre dois componentes:\n",
    "\n",
    "- **Recuperação (Retriever):** busca informações relevantes em documentos externos (como PDFs, textos ou bancos de dados vetoriais).\n",
    "- **Geração (Generator):** o modelo de linguagem usa essas informações, junto com seu próprio conhecimento, para produzir uma resposta precisa e contextualizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Como o processo funciona na prática:\n",
    "\n",
    "1. A pessoa usuária faz uma pergunta inicial.\n",
    "2. O sistema identifica quais informações são relevantes em fontes externas. \n",
    "3. As informações relevantes são passadas como contexto. \n",
    "4. O contexto é juntado com a entrada inicial do usuário e passada para a LLM.\n",
    "5. O modelo de linguagem analisa tudo e gera uma resposta integrada.\n",
    "\n",
    "    <img src=\"https://i.imgur.com/dyVzasZ.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conceitos fundamentais:\n",
    "\n",
    "- **Langchain:** framework especializado em coordenar o uso de LLMs, simplificando o desenvolvimento de aplicações complexas com modelos de linguagem, como a RAG.\n",
    "- **Chunking:** divisão de textos longos em pedaços menores (*chunks*), o que torna a busca mais eficiente e garante que os textos estejam no tamanho limite de contexto do modelo.\n",
    "- **Embeddings:** representações vetoriais dos textos, usadas para medir a similaridade entre uma pergunta e os documentos. Os embeddings são uma forma de buscar por significado. \n",
    "- **Vector Store:** banco vetorial onde são armazenados os embeddings dos documentos. É ele que permite a recuperação rápida e eficiente dos chunks mais relevantes durante uma consulta.\n",
    "- **LCEL:** é sigla para Langchain Expression Language, que é uma forma modular, declarativa e encadeável de integrar todas as etapas do pipeline no Langchain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fontes:`\n",
    "- O que é RAG?: https://www.alura.com.br/artigos/o-que-e-rag#:~:text=RAG%20%28Retrieval%20Augmented%20Generation%29%20%C3%A9%20uma%20t%C3%A9cnica%20utilizada,de%20linguagem%20com%20sistemas%20de%20recupera%C3%A7%C3%A3o%20de%20informa%C3%A7%C3%B5es.\n",
    "\n",
    "- RAG: https://medium.com/blog-do-zouza/rag-retrieval-augmented-generation-8238a20e381d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> A seguir, vamos explorar cada etapa necessária para construir a nossa pipeline de RAG usando o Langchain. Antes, **leia o README.md** para realizar o *setup*  do ambiente e entender a configuração básica desse *template*.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalações:\n",
    "\n",
    "Antes de tudo, é necessário instalar as bibliotecas que serão usadas. Como explicado no README.md, faremos isso pelo arquivo requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -r requirements.txt\n",
    "%pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **`1`** - **Importações**\n",
    "\n",
    "Depois de instalar as bibliotecas, vamos importar os módulos necessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os # biblioteca padrão do Python para interagir com o sistema operacional (acessar variáveis do ambiente, trabalhar com chaves de API, ou com caminhos de arquivos). \n",
    "from langchain_openai import OpenAIEmbeddings # gerador de embeddings da OpenAI, convertendo texto em vetores numéricos.\n",
    "from langchain_community.document_loaders import PyPDFLoader # carregar conteúdo de arquivos PDF, convertendo para um formato legível pelo LangChain.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # ferramenta de chunking, divide o texto em partes menores com sobreposições.\n",
    "from langchain_core.documents import Document # define a estrutura básica de um documento em Langchain, que contém texto e metadados.\n",
    "from langchain_core.vectorstores import InMemoryVectorStore # implementação de armazenamento vetorial em memória, permitindo a busca por similaridade.\n",
    "from langchain_core.prompts import ChatPromptTemplate # usado para criar prompts personalizados, combinando variáveis, contexto e perguntas dentro de um único template estruturado. \n",
    "from langchain_core.output_parsers import StrOutputParser # converter a resposta gerada pelo modelo em uma string limpa\n",
    "from langchain_core.runnables import RunnablePassthrough # componente auxiliar usado em LCEL para passar os dados adiante sem alterações, conectando partes do pipeline que ainda não precisam ser processadas. \n",
    "from langchain.chains import create_retrieval_chain # cria a cadeia de recuperação da RAG, desde a entrada do usuário até o output final.\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain # cria uma cadeia de geração (stuffing) onde os documentos recuperados são empacotados no prompt para gerar a resposta.\n",
    "\n",
    "from dotenv import load_dotenv # biblioteca para carregar variáveis de ambiente de um arquivo .env, facilitando a configuração de chaves de API e outras configurações sensíveis.\n",
    "load_dotenv() # chama a função load_dotenv para carregar as variáveis de ambiente do arquivo .env.\n",
    "\n",
    "# Recupera a variável de ambiente OPENAI_API_KEY e armazena esse valor na variável API_KEY_OPENAI.\n",
    "API_KEY_OPENAI = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Coloca a chave da API manualmente no ambiente do sistema operacional, garantindo que tenha o valor certo e que funcione. \n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY_OPENAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## **`2`** - **Carregamento dos Dados (Data Injection)**\n",
    "\n",
    "Nesta etapa, carregamos o conteúdo que será usado como base de conhecimento do nosso sistema RAG.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nosso exemplo, vamos trabalhar com um artigo científico sobre RAGs, em formato PDF. O conteúdo no arquivo PDF será transformado em objetos do tipo `Document`, que são estruturas usadas internamente pela LangChain para organizar o texto e suas informações associadas (como o nome do arquivo ou número da página).\n",
    "\n",
    "\n",
    "- Criar **um `Document` por página**, quando o conteúdo faz sentido separadamente\n",
    "- **Concatenar todas as páginas** em um único `Document`, quando o conteúdo depende de contexto global\n",
    "\n",
    "> A seguir, carregamos o PDF, extraímos o texto de cada página, e juntamos tudo em um único `Document` com metadados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Injection com um PDF\n",
    "\n",
    "loader = PyPDFLoader(\"./meus_arquivos/RAG_LLM.pdf\") # Cria um loader para ler o arquivo. Substitua pelo seu caminho do PDF.\n",
    "pages = loader.load_and_split() # Carrega o PDF e divide o conteúdo em páginas, onde cada uma vira um Document. Agora, Pages é uma lista de objetos Document (um Document equivale a uma pagina do PDF).\n",
    "\n",
    "pdf_source = pages[0].metadata['source'] # Da primeira pagina do documento, pega o metadado 'source', que é o caminho do arquivo PDF. Será usado como a fonte do seu documento final concatenado. \n",
    "\n",
    "list_document = [] # Cria uma lista vazia para armazenar o documento final concatenado. \n",
    "\n",
    "# Junta o conteúdo de todas as páginas em um único texto, separando cada página por seis quebras de linha.\n",
    "concatenated_text = \"\\n\\n\\n\\n\\n\\n\".join(\n",
    "    [page.page_content for page in pages])\n",
    "# Cria um novo objeto Document com o conteúdo concatenado do PDF inteiro e o metadado 'source' que indica a origem do PDF.\n",
    "document = Document(\n",
    "    metadata={'source': pdf_source}, page_content=concatenated_text)\n",
    "# Adiciona o Document concatenado à lista de documentos.\n",
    "list_document.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`E se forem múltiplos documentos?`**\n",
    "\n",
    "Para carregar múltiplos arquivos, é necessário implementar um for loop que percorre a lista, aplicando o mesmo processo para cada arquivo dela:\n",
    "\n",
    "- Cria um loader\n",
    "\n",
    "- Carrega o PDF e divide o conteúdo em páginas\n",
    "\n",
    "- Pega o metadado da primeira página do documento\n",
    "\n",
    "- Junta o conteúdo de todas as páginas em um único texto\n",
    "\n",
    "- Cria um novo objeto Document com o conteúdo concatenado do PDF inteiro e adiciona à list_document\n",
    "    \n",
    "- OBS: essa list_document tem que ser criada fora do loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Injection com vários PDFs\n",
    "\n",
    "list_document = [] # Cria uma lista vazia para armazenar o documento final concatenado. \n",
    "pasta = \"./meus_arquivos\" # Define o caminho da pasta onde estão os arquivos PDF.\n",
    "pdf_path = [os.path.join(pasta, f) for f in os.listdir(pasta) if f.endswith('.pdf')] # Cria uma lista contendo os caminhos de todos os arquivos PDF na pasta especificada.\n",
    "\n",
    "for i in pdf_path: # Para cada arquivo PDF na lista pdf_path:\n",
    "    loader = PyPDFLoader(i) # Cria um loader para ler o arquivo\n",
    "    pages = loader.load_and_split() # Carrega o PDF e divide o conteúdo em páginas, onde cada uma vira um Document. Agora, Pages é uma lista de objetos Document (um Document equivale a uma página do PDF).\n",
    "\n",
    "    pdf_source = pages[0].metadata['source'] # Da primeira página do documento, pega o metadado 'source', que é o caminho do arquivo PDF. Será usado como a fonte do seu documento final concatenado. \n",
    "\n",
    "\n",
    "    # Junta o conteúdo de todas as páginas em um único texto, separando cada página por seis quebras de linha.\n",
    "    concatenated_text = \"\\n\\n\\n\\n\\n\\n\".join(\n",
    "        [page.page_content for page in pages])\n",
    "    # Cria um novo objeto Document com o conteúdo concatenado do PDF inteiro e o metadado 'source' que indica a origem do PDF.\n",
    "    document = Document(\n",
    "        metadata={'source': pdf_source}, page_content=concatenated_text)\n",
    "    # Adiciona o Document concatenado à lista de documentos.\n",
    "    list_document.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`MetaData: O que é, Como Tratamos e Por que precisamos?`**\n",
    "\n",
    "\n",
    "Como vimos na seção de **Data Injection**, ao carregar documentos (como arquivos PDF), cada página ou trecho de texto é transformado em um objeto `Document`. Esses objetos não carregam apenas o conteúdo textual, eles também trazem consigo um campo muito importante: a `metadata`.\n",
    "\n",
    "##### **O que é Metadata?**\n",
    "\n",
    "**Metadata** são **informações adicionais sobre o conteúdo**, como:\n",
    "- O nome do arquivo de origem ('source')\n",
    "- O número da página original ('page')\n",
    "- Qualquer outra informação contextual que você queira adicionar (ex: autor, categoria, data)\n",
    "\n",
    "Exemplo de **Document** com metadata:\n",
    "```\n",
    "Document(\n",
    "    page_content=\"Texto da página 1...\",\n",
    "    metadata={\n",
    "        'source': 'contrato.pdf',\n",
    "        'page': 0\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "##### **Como tratamos no projeto?**\n",
    "Durante o carregamento e pré-processamento dos documentos:\n",
    "\n",
    "- Cada página lida com o PyPDFLoader já vem com metadados automaticamente\n",
    "\n",
    "- Quando concatenamos páginas em um único Document, preservamos o source\n",
    "\n",
    "- No chunking, os pedaços menores herdam automaticamente os metadados do documento original\n",
    "\n",
    "**Isso é importante porque permite rastrear a origem da informação utilizada na resposta, contribui para a construção de respostas mais confiáveis e transparentes, possibilita a exibição da fonte (como “Fonte: guia.pdf”) e viabiliza filtros por tipo de documento em consultas mais avançadas.**\n",
    "\n",
    "\n",
    "`Fontes:`\n",
    "\n",
    "\n",
    "- Carregamento de Dados de grandes bancos de dados: https://www.crossml.com/build-a-rag-data-ingestion-pipeline/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **`3`** - **Chunking (Dividindo os Documentos em Pedaços Menores)**\n",
    "\n",
    "Depois de carregar os documentos, o próximo passo é o chunking que consiste em dividir textos longos, como os Documents que acabamos de gerar, em pedaços menores chamados de chunks. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Essa etapa é fundamental porque:**\n",
    "- LLMs possuem um limite de tokens por entrada, então o texto longo inteiro não seria suportado pelo modelo.\n",
    "- Menos é mais: trabalhar com pedaços menores melhora a performance da recuperação de contexto.\n",
    "- Preserva a coerência e relevância durante a resposta, já que cada pedaço representa uma unidade de informação.\n",
    "\n",
    "#### **`Como fazemos o chunking?`**\n",
    "\n",
    "Utilizamos uma ferramenta do Langchain chamada \"RecursiveCharacterTextSplitter\" que tenta quebrar os textos com base em \"separadores hierárquicos\", estabelece um tamanho máximo para cada chunk e mantém uma sobreposição entre os pedaços para preservar contexto entre eles. \n",
    "\n",
    "#### **`Mas o que são separadores hierárquicos, e como escolhemos eles?`**\n",
    "\n",
    "São \"símbolos de quebra\" usados para tentar dividir o texto de forma natural e lógica, sem quebra de palavras. \n",
    "No código abaixo, usamos a lista default de separadores:\n",
    "\n",
    "(\"\\n\\n\", \"\\n\", \" \", \"\")\n",
    "\n",
    "\n",
    "##### **O processo funciona assim:**\n",
    "\n",
    "1. Primeiro tenta dividir por \\n\\n (duas quebras de linha — parágrafo)\n",
    "\n",
    "2. Se não der, tenta \\n (quebra de linha simples)\n",
    "\n",
    "3. Depois por \" \"(fim de frase)\n",
    "\n",
    "4. E se não conseguir mais dividir logicamente, a ferramenta realiza a quebra forçada (\"\") de acordo com o tamanho máximo (chunk_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Como escolher bons separadores?**\n",
    "\n",
    "O tipo de separadores que escolhemos varia de acordo com o **tipo de documento que estamos carregando**.\n",
    "É importante considerar a **estrutura natural do texto**, os **pontos lógicos de separação de ideias**, e o **formato dos dados**.\n",
    "Algumas instruções no processo de escolha dos seus separadores são: \n",
    "\n",
    "- Se o texto estiver dividido em parágrafos visíveis, use \\n\\n como separador principal.\n",
    "- Se cada linha for uma unidade (como listas, código ou logs), use \\n como separador principal. \n",
    "- A sequência de lista default (\"\\n\\n\", \"\\n\", \" \", \"\") geralmente funciona melhor para arquivos e PDFs.\n",
    "- Um bom separador deve dividir o texto em blocos que façam sentido isoladamente, sem cortar frases importantes no meio.\n",
    "- Use uma lista ordenada de separadores do mais forte (parágrafos) ao mais fraco (palavras).\n",
    "- Se está em dúvida, comece com a sequência default e vá refinando de acordo com os resultados.\n",
    "\n",
    "\n",
    "\n",
    "#### **`Tamanho Máximo (chunk_size) com Sobreposição: O que é, e como defini-lo?`**\n",
    "\n",
    "É o tamanho máximo (em caracteres) que cada chunk pode ter, garantindo que o texto fique curto o suficiente para ser processado, mas longo o bastante para manter o contexto.\n",
    "\n",
    "##### **Como definir um chunk_size ideal?**\n",
    "\n",
    "O chunk_size ideal depende de três fatores principais:\n",
    "\n",
    "- O limite de contexto do modelo de linguagem que você está usando\n",
    "\n",
    "- A estrutura e densidade do conteúdo dos seus documentos\n",
    "\n",
    "- O objetivo da aplicação (responder perguntas, resumir, classificar, etc)\n",
    "\n",
    "O ideal é analisar o seu documento e definir um parâmetro inicial, e, a partir daí testar com diferentes tamanhos no seu conteúdo real, verificar quantos chunks são gerados, qual a qualidade da recuperação e se há cortes abruptos em frases ou parágrafos, e ir ajustando até achar o tamanho ideal para o seu documento, ou para o seu tipo de documento. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **`E a sobreposição (chunk_overlap)?`**\n",
    "\n",
    "A sobreposição (chunk_overlap) define quantos caracteres do final de um chunk serão repetidos no início do próximo. Essa repetição é importante para preservar a continuidade das ideias, principalmente quando uma informação começa no final de um chunk e continua no seguinte.\n",
    "Sem essa sobreposição, o modelo pode interpretar os pedaços como blocos desconectados, prejudicando a precisão da recuperação e da geração de respostas.\n",
    "\n",
    "Por exemplo, se chunk_size = 300 e chunk_overlap = 50, os chunks serão criados mais ou menos assim:\n",
    "\n",
    "- Chunk 1 → caracteres 0 a 300\n",
    "\n",
    "- Chunk 2 → caracteres 250 a 550\n",
    "\n",
    "- Chunk 3 → caracteres 500 a 800\n",
    "\n",
    "Note que os últimos 50 caracteres do chunk anterior reaparecem no início do próximo, garantindo que o modelo mantenha o fio da narrativa ou explicação.\n",
    "\n",
    "\n",
    "##### **Como definir um chunk_overlap ideal?**\n",
    "\n",
    "A escolha ideal do chunk_overlap depende do nível de dependência entre as frases ou trechos do seu documento: quanto mais um trecho depende do anterior para fazer sentido, maior deverá ser o chunk_overlap. \n",
    "Analogamente à escolha do chunk_size,  O ideal é analisar o seu documento e definir um parâmetro inicial, e, a partir daí testar com diferentes tamanhos no seu conteúdo real, verificar qual a qualidade da recuperação e se há cortes abruptos em frases ou parágrafos, e ir ajustando até achar o tamanho ideal de sobreposição para o seu tipo de documento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 200 # Define o tamanho máximo de cada chunk para 200 caracteres.\n",
    "chunk_overlap = 100 # Define a sobreposição entre os chunks como 100 caracteres.\n",
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\"] # DEfine os separadores hierárquicos\n",
    "\n",
    "# Cria um objeto 'text_splitter' que vai gaurdar as regras para cortar os documentos em pedaços menores (chunks), usando a ferramenta do Langchain 'RecursiveCharacterTextSplitter\n",
    "# Esse divisor é recursivo: tenta usar os separadores em ordem sem ultrapassar o limite do chunk_size.\n",
    "text_splitter = RecursiveCharacterTextSplitter( # A divisão respeita o chunk_size, o chunk_overlap e os separadores hierárquicos definidos.\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap, \n",
    "    separators=separators\n",
    "    )\n",
    "# As regras da divisão definidas em 'text_splitter' são executadas por 'split.documents' na list_documents, e os pedaços da divisão (chunks) são armazenados na variável 'chunks'.\n",
    "chunks = text_splitter.split_documents(list_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fontes:` \n",
    "\n",
    "- Guia Definitivo de Chunking: https://www.robertodiasduarte.com.br/guia-definitivo-de-chunking-para-rag-e-llms-estrategias-essenciais/\n",
    "- Five Levels of Chunking Strategies: https://medium.com/@anuragmishra_27746/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d\n",
    "- Guide for Chunking Phase: https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-chunking-phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`4`** - **Vector Store - Criação e Armazenamento de Embeddings**\n",
    "\n",
    "Agora que dividimos os documentos em chunks, o próximo passo é transformar esses pedaços em embeddings (vetores numéricos), e armazená-los em uma estrutura chamada 'Vector Store'.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`O que é uma vector store?`**\n",
    "\n",
    "Uma Vector Store é um  banco de dados que armazena embeddings (vetores numéricos) junto com seus textos originais. Ela permite que, ao receber uma nova pergunta, possamos transformá-la em vetor e **buscar os chunks mais parecidos** com base em similaridade vetorial.\n",
    "\n",
    "#### **`Mas primeiro: O que são embeddings, e como os criamos?`**\n",
    "\n",
    "Embeddings são vetores numéricos que representam textos (palavras, frases ou parágrafos) de forma que **proximidade entre vetores reflita proximidade de significado**. São valores matemáticos, transformados de linguagem humana, após serem processados por algoritmos, \n",
    "\n",
    "**`Exemplo:`** os embeddings de “carro” e “automóvel” estarão próximos no espaço vetorial, mesmo com palavras diferentes.\n",
    "\n",
    "Para gerar os embeddings, usamos um modelo de embedding da OpenAI chamado \"text-embedding-3-small\", que transforma cada chunk em um vetor denso de alta dimensionalidade. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Como o modelo de embedding da OpenAI foi criado?**\n",
    "\n",
    "Existem vários modelos de embeddings, como esse da OpenAI que estamos usando, que são modelos de machine learning, criados por meio de redes neurais treinadas em grandes volumes de texto.\n",
    "\n",
    "> Vamos entender como esse modelo de Embedding da OpenAI foi criado\n",
    "\n",
    "O modelo é alimentado com grandes volumes de texto e executa a tarefa de previsão automática da palavra ausente em uma frase.\n",
    "    Exemplo: \"O céu está ___\" → o modelo tenta prever \"azul\".\n",
    "\n",
    "O processo para realizar essa tarefa de treinamento ocorre da seguinte forma:\n",
    "\n",
    "1. **Processamento do texto pelas camadas neurais:** O texto é convertido em números (tokens) e passa por várias camadas diferentes da rede neural, onde cada camada extrai relações e padrões mais complexos (como estrutura gramatical, significado, tom e contexto).\n",
    "\n",
    "2. **Geração de um vetor interno (embedding)**: Antes da camada final, onde a palavra ausente é escolhida, é gerada uma representação vetorial daquele texto, chamada embedding, que representa o significado do texto. \n",
    "\n",
    "3. **Cálculo do erro:** Na camada final, onde é feita a previsão, o modelo compara a palavra prevista com a real, e faz o cálculo do erro (função de perda).\n",
    "\n",
    "4. **Backpropagation e Ajuste dos Pesos:** Com o erro final calculado, entra o processo de backpropagation, em que o modelo vai camada por camada e determina quais conexões (pesos) em cada camada ajudaram ou atrapalharam a chegar na resposta errada ao calcular o gradiente, que o diz basicamente se deve aumentar ou diminuir o peso, e quanto, e vai ajustando os pesos conforme o gradiente. Isso acontece milhares de vezes até o modelo aprender.\n",
    "\n",
    "5. **Aproximação de significados semelhantes no espaço vetorial:** Após o treinamento, o modelo é capaz de gerar embeddings (vetores) parecidos para textos com significados parecidos.\n",
    "Por exemplo, frases como \"o carro está na garagem\" e \"o automóvel está estacionado\" terão embeddings próximos.\n",
    "\n",
    "Após o treinamento, o modelo é usado diretamente, como estamos fazendo aqui: envia um texto, e retorna o embedding, um vetor que representa seu significado em um espaço matemático de alta dimensão. \n",
    "\n",
    "**`ATENÇÃO`**: A explicação dada acima se refere unicamente ao treinamento do modelo de Embedding da OpenAI que usamos nesse template. Existem outras formas de criar um modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`E por que usamos Embeddings?`**\n",
    "\n",
    "Em uma pipeline RAG, queremos que o modelo de linguagem gere respostas com base em **informações relevantes e específicas** dos nossos documentos.  \n",
    "Para isso, precisamos de uma forma de **buscar os trechos mais parecidos** com a pergunta feita pelo usuário — e é aqui que os **embeddings entram**.\n",
    "\n",
    "##### **Problemas com buscas tradicionais**\n",
    "\n",
    "Um sistema de busca baseado em palavras-chave (como Crtl-F)  **só encontra trechos com os mesmos termos exatos**, e não tem suporte para  **sinônimos, variações e perguntas indiretas**, comuns no contexto de perguntas do usuário. Portanto, esse sistema falha porque não entende o significado.\n",
    "\n",
    "\n",
    "Embeddings resolvem esse problema ao **converter textos em vetores numéricos que capturam seu significado**.  \n",
    "Com isso, conseguimos:\n",
    "\n",
    "- Converter a **pergunta do usuário em um vetor**\n",
    "- Comparar com os **vetores dos chunks armazenados**\n",
    "- Retornar os **mais semelhantes semanticamente**\n",
    "\n",
    "\n",
    "##### **Onde os embeddings são usados? No Retriever.**\n",
    "\n",
    "O **retriever** é o componente da pipeline RAG responsável por:\n",
    "\n",
    "1. Receber a pergunta do usuário\n",
    "2. Gerar seu embedding (vetor)\n",
    "3. Comparar esse vetor com os embeddings dos chunks (na Vector Store)\n",
    "4. Trazer os chunks mais próximos no espaço vetorial\n",
    "\n",
    "Ou seja, o retriever **usa embeddings para encontrar o conteúdo mais relevante**, mesmo que a pergunta e o documento usem palavras diferentes.\n",
    "\n",
    "\n",
    "\n",
    "O retriever precisa **entender o contexto da pergunta**, e não apenas bater palavras exatas.  \n",
    "Embeddings tornam isso possível, permitindo buscas por **significado**, e não só por termos.  \n",
    "Isso melhora a precisão, a flexibilidade e a capacidade de resposta da aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **E como comparamos embeddings?**\n",
    "\n",
    "Para saber **quais embeddings estão mais próximos**, usamos uma métrica chamada **similaridade por cosseno** (*cosine similarity*).\n",
    "\n",
    "Essa métrica mede o **ângulo entre dois vetores**, e o valor da similaridade varia de:\n",
    "\n",
    "- `1` → vetores idênticos (máxima similaridade)\n",
    "- `0` → vetores completamente diferentes\n",
    "- `-1` → vetores opostos (raro em embeddings de texto)\n",
    "\n",
    "Essa técnica permite recuperar, de forma matemática e precisa, **os chunks mais semanticamente próximos da pergunta**.\n",
    "\n",
    "**`ATENÇÃO`**: Há outras formas de comparar os embeddings, mas nesse *template* usamos o método de similaridade por cosseno, ou *cosine similarity*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingsmodel = OpenAIEmbeddings(model=\"text-embedding-3-small\") # Cria a variável 'embeddingsmodel' que acessa o modelo de embedding da OpenAI por meio da classe OpenAIEmbeddings, que te conecta com a API da OpenAI. \n",
    "vectorstore = InMemoryVectorStore(embeddingsmodel) # Cria a variável \"vectorstore\" que representa um objeto da classe \"InMemoryVectorStore\", que chama a variável criada acima para usar o modelo de embeddings, gerar vetores a partir dos chunks gerados anteriormente, e armazena-os na memória. \n",
    "_ = vectorstore.add_documents(documents=chunks) # Adiciona os chunks de texto a Vector Store, possibilitando buscas por similaridade entre os chunks de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Teste a busca por similaridade`**\n",
    "\n",
    "A célula a seguir serve como um teste manual da etapa de vector store e busca por similaridade, não faz parte da pipeline RAG em produção.\n",
    "\n",
    "É uma forma de validar, antes de criar o retriever e a chain, que a base vetorial está pronta e retornando resultados coerentes.\n",
    "\n",
    "\n",
    "**`ATENÇÃO`**: É recomendado que as perguntas de teste sejam feitas em inglês, para haver perda mínima de contexto em uma tradução.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b8a9b11c-0561-4ef2-8bd6-659a90d1312e', metadata={'source': './meus_arquivos\\\\RAG_LLM.pdf'}, page_content='overcome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-'),\n",
       " Document(id='e64ce71f-3a0e-4f9e-91ef-63ce7f165a29', metadata={'source': './meus_arquivos\\\\RAG_LLM.pdf'}, page_content='of developing specialized strategies to integrate retrieval with\\nlanguage generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.'),\n",
       " Document(id='206aee48-cd0f-4ec8-8b74-b3ac1873815f', metadata={'source': './meus_arquivos\\\\RAG_LLM.pdf'}, page_content='3\\nFig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,'),\n",
       " Document(id='46f523eb-37e1-4391-9f38-61fd2f9a022a', metadata={'source': './meus_arquivos\\\\RAG_LLM.pdf'}, page_content='to the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-'),\n",
       " Document(id='6afdfb9c-0c9d-4c00-a010-f5e12b018df8', metadata={'source': './meus_arquivos\\\\RAG_LLM.pdf'}, page_content='responses, thus improving the overall efficiency of the RAG\\nsystem. To capture the logical relationship between document\\ncontent and structure, KGP [91] proposed a method of building')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faz uma busca por similaridade. A pergunta é transformada em um vetor (embedding), e esse vetor é comparado com todos os vetores armazenados na Vector Store (os chunks). Ele calcula a similaridade cosseno entre eles, e retorna os 5 chunks mais semelhantes a pergunta, em termos semânticos.\n",
    "vectorstore.similarity_search(\n",
    "    \"How does the RAG method combine document retrieval with natural language generation?\",\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "# Colocar para salvar o vectorstore localmente para não ficar rodando o modelo de embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fontes:`\n",
    "\n",
    "- Guia para gerar embeddings: https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-generate-embeddings\n",
    "\n",
    "- Como selecionar um modelo de embedding: https://galileo.ai/blog/mastering-rag-how-to-select-an-embedding-model\n",
    "\n",
    "- Embeddings e Vetorização: https://joaomarcuraa.medium.com/rag-embeddings-e-vetoriza%C3%A7%C3%A3o-potencializando-a-ia-com-python-e704a39699dd\n",
    "\n",
    "- Vector Search e Embeddings: https://www.thecloudgirl.dev/blog/the-secret-sauce-of-rag-vector-search-and-embeddings\n",
    "\n",
    "- Embeddings: https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "- Modelo de Embedding da OpenAI: https://platform.openai.com/docs/guides/embeddings#embedding-models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`5`** - **Retriever**\n",
    "\n",
    "\n",
    "Agora que já temos todos os nossos **chunks armazenados na Vector Store com seus embeddings**, o próximo passo é preparar o **componente que será responsável por buscar automaticamente os trechos relevantes de texto** quando o usuário fizer uma pergunta: o **Retriever**.\n",
    "\n",
    "**`ATENÇÃO`**: O teste acima fazendo essa busca pela ferramenta similarity_search foi **manual**. Agora essa busca será automatizada e feita internamente pelo retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Relembrando: O que é Retriever?`**\n",
    "\n",
    "\n",
    "O **Retriever** é uma camada que se conecta à Vector Store para realizar buscas **semânticas** (por significado), e **retornar os chunks mais relevantes** com base em uma consulta textual.\n",
    "\n",
    "> Ele é a ponte entre a pergunta do usuário e os pedaços de texto armazenados que mais se aproximam dessa pergunta em termos de conteúdo.\n",
    "\n",
    "Enquanto a Vector Store **armazena** os embeddings, o Retriever **faz a busca neles**, de forma otimizada.\n",
    "\n",
    "**`ATENÇÃO`**:Há vários tipos de retrievers (Naive, Parent Document, Self-Query, Contextual Comprehension), mas nesse template estamos implementando o Naive Retriever, o mais simples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma variavel 'retriever' que acessa a vector store e aplica o método 'as_retriever' para transformar a Vector Store em um objeto (retriever). \n",
    "# O parâmetro search_kwargs={\"k\": 5} define que, a cada consulta, ele deve retornar os 5 chunks mais semelhantes.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fontes:`\n",
    "\n",
    "- Técnicas de Retriever: https://towardsdatascience.com/advanced-retriever-techniques-to-improve-your-rags-1fac2b86dd61/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`6`** - **Geração de Prompt - Montando a pergunta com contexto**\n",
    "\n",
    "Agora que já temos nosso **retriever configurado**, pronto para buscar automaticamente os **chunks mais relevantes** da base vetorial com base na pergunta do usuário, o próximo passo é **preparar a entrada que será enviada ao modelo de linguagem** (LLM).\n",
    "\n",
    "Esse passo é fundamental: o **modelo precisa receber tanto o contexto quanto a pergunta**, organizados de forma clara, para gerar uma **resposta precisa e fiel** ao conteúdo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Mas afinal, o que é prompt?`**\n",
    "\n",
    "Um prompt é o texto de entrada que você envia para um modelo de linguagem (LLM), como o da OpenAI.\n",
    "\n",
    "Ele serve para instruir o modelo sobre o que ele deve fazer e qual informação ele deve usar para gerar uma resposta.\n",
    "\n",
    "O modelo de linguagem não “sabe” o que fazer sozinho — ele só reage ao que está no prompt.\n",
    "Se o prompt for mal formulado, ele pode:\n",
    "\n",
    "- Ignorar o contexto\n",
    "\n",
    "- Inventar respostas\n",
    "\n",
    "- Responder de forma genérica\n",
    "\n",
    "Um bom prompt garante que o modelo use o contexto corretamente e responda com precisão.\n",
    "\n",
    "\n",
    "#### **`E o que vamos fazer com ele agora?`**\n",
    "\n",
    "Vamos criar um **prompt estruturado**, ou seja, um *template* que:\n",
    "\n",
    "prompt completo:\n",
    "\n",
    "Seu prompt é composto por:\n",
    "\n",
    "- Mensagem de sistema → define o comportamento do modelo (que não deve ser apresentado ao usuário final)\n",
    "\n",
    "- Mensagem do usuário → traz a pergunta\n",
    "\n",
    "- Contexto recuperado → alimentado dentro do template\n",
    "\n",
    "Primeiro importamos a ferramenta do Langchain 'ChatOpenAI' que permite usar os modelos de chat da OpenAI dentro do Langchain, se conectando à API da OpenAI, enviando o prompt, e recebendo a resposta. \n",
    "\n",
    "\n",
    "\n",
    "##### **Importando ChatOpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importa a ferramenta que permite usar os modelos de chat da OpenAI dentro do Langchain (conectar a API, envia um prompt e recebe a resposta)\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Criando o Modelo de Prompt**\n",
    "\n",
    "Como vimos acima, o modelo de prompt é composto pela mensagem de sistema, pela mensagem do usuário, e pelo contexto recuperado (do retriever). \n",
    "\n",
    "**`ATENÇÂO`**: A mensagem de sistema normalmente é adaptada de acordo com o documento e com o objetivo da aplicação, e não deve ser apresentada ao usuário final. \n",
    "\n",
    "**Etapas:**\n",
    "\n",
    "1. Primeiro, vamos criar a 'system_prompt' para guardar a mensagem de sistema e o contexto recuperado. \n",
    "\n",
    "2. Depois, vamos juntar esse 'system_prompt' com a entrada do usuário e guardá-lo em 'prompt'. Faremos isso pela ferramenta 'ChatPromptTemplate', que serve para montar os prompts no formato de mensagens, permitindo criar um template de prompt com partes variáveis.\n",
    "\n",
    "3. Seguindo o formato de mensagem, definimos 'system' como 'system_prompt', representando o papel e o comportamento da IA. E definimos 'human' como a pergunta feita pelo usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma string longa chamada \"system_prompt\" que será usada como mensagem do sistema, uma das partes do prompt, que define o comportamento e papel do modelo durante a resposta.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks for compliance documents. \" # Define o papel da IA\n",
    "    \"Use the following pieces of retrieved context to answer \" # Diz de onde a IA deve buscar as informações para responder, além de seu conhecimento interno\n",
    "    \"the question. If you don't know the answer, say that you \" # Uma instrução de segurança para impedir que a IA invente respostas\n",
    "    \"don't know. Keep the answer as close as retrieved context as possible.\" # Outra instrução de segurança para impedir que a IA invente respostas\n",
    "    \"\\n\\n\"\n",
    "    \"Context: {context}\" # Espaço para inserir o contexto recuperado, que são os chunks mais relevantes para a pergunta feita, obtido pelo retriever.\n",
    ")\n",
    "\n",
    "# Juntando o system_prompt (contendo a mensagem do sistema e o contexto) com a pergunta do usuário pelo ChatPromptTemplate, ferramenta  que permite criar prompts estruturados para modelos.\n",
    "prompt = ChatPromptTemplate.from_messages( # o 'from_messages' é um método dessa ferramenta que cria um template de prompt a partir de uma lista de mensagens.\n",
    "    [\n",
    "        (\"system\", system_prompt), # A primeira mensagem é do tipo \"system\", que define o papel da IA e o contexto que ela deve usar para responder.\n",
    "        (\"human\", \"Question: {input}\"), # A segunda mensagem é do tipo \"human\", que representa a pergunta feita pelo usuário. O \"{input}\" será substituído pela pergunta real.\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Teste da construção final do prompt**\n",
    "\n",
    "Nessa etapa vamos testar se o prompt foi construído corretamente antes de enviá-lo para o modelo de linguagem (LLM). Isso inclui verificar se:\n",
    "- O contexto recuperado está no lugar certo.\n",
    "- A pergunta do usuário está no lugar certo.\n",
    "- O formato da mensagem está seguindo a estrutura esperada. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are an assistant for question-answering tasks for compliance documents. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Keep the answer as close as retrieved context as possible.\n",
      "\n",
      "Context: overcome challenges, Retrieval-Augmented Generation (RAG)\n",
      " enhances LLMs by retrieving relevant document chunks from\n",
      "external knowledge base through semantic similarity calculation.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Question: How does the RAG method combine document retrieval with natural language generation?\n"
     ]
    }
   ],
   "source": [
    "# Chamamos o prompt com o método 'invoke', passando um dicionário com as chaves \"context\" e \"input\".\n",
    "# A chave \"context\" recebe o contexto recuperado pelo retriever, e a chave \"input\" recebe a pergunta do usuário, e as duas são salvas em 'message_list'.\n",
    "message_list = prompt.invoke({\n",
    "    \"context\": \"overcome challenges, Retrieval-Augmented Generation (RAG)\\n enhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calculation.\",\n",
    "    \"input\": \"How does the RAG method combine document retrieval with natural language generation?\"\n",
    "})\n",
    "\n",
    "# Itera sobre a lista de mensagens retornadas pelo prompt e chama o método 'pretty_print' para exibir cada mensagem de forma legível.\n",
    "for msg in message_list.messages:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fontes:`\n",
    "\n",
    "- Engenharia de Prompt em RAG: https://ventiladigital.com.br/blog/estrategias-eficazes-para-engenharia-de-prompts-em-rag/\n",
    "\n",
    "- Guia de Prompt - Tecnicas para RAG: https://www.promptingguide.ai/pt/techniques/rag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`7`** - **RAG Chain**\n",
    "\n",
    "Chegamos à etapa final da pipeline: a RAG Chain. Esse é o componente que integra todas as partes da pipeline:\n",
    "- Retriever: busca os chunks mais relevantes para o contexto\n",
    "- Prompt: organiza o contexto, a mensagem de sistema e a pergunta do usuário em um só. \n",
    "- LLM: gera a resposta final\n",
    "\n",
    "Aqui é onde o sistema pega a pergunta do usuário, recupera o contexto, monta o prompt, e gera a resposta de forma totalmente automatizada. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo vai funcionar da seguinte forma:\n",
    "\n",
    "1. Primeiro vamos **escolher o modelo e definir alguns parametros**.\n",
    "2. Depois vamos **conectar os componentes da pipeline, e obter a resposta final da LLM**. Podemos fazer isso de dois modos diferentes, dependendo do nível de personalização necessário na sua aplicação:\n",
    " - **LCEL**: uma forma manual e flexível de montar a pipeline. Você conecta componentes usando o operador |, como blocos funcionais encadeados. Ideal para quem precisa de **mais controle**, quer modificar partes da lógica, integrar validações ou adicionar etapas intermediárias.\n",
    " - **Create Stuff**: usando funções utilitárias como 'create_stuff_documents_chain' e 'create_retrieval_chain(retriever, doc_chain)', que já **montam e conectam os blocos principais automaticamente**, o que reduz código e acelera o desenvolvimento, sem muita customização e controle. \n",
    "\n",
    "**`E quando devo usar LCEL, e quando devo usar Create Stuff?`**\n",
    "\n",
    "| Caso de uso                                      | Recomendação     |\n",
    "|--------------------------------------------------|--------------------|\n",
    "| Preciso de controle sobre cada etapa da pipeline | **LCEL**          |\n",
    "| Quero alterar o parser, prompt ou fluxo de dados | **LCEL**         |\n",
    "| Estou fazendo um protótipo funcional             | **Create Stuff**  |\n",
    "| O projeto é simples e precisa de pouco código    | **Create Stuff**  |\n",
    "| Quero lógica personalizada (ex: filtros, logs)   | **LCEL**          |\n",
    "| Quero escalar com menos esforço manual           | **Create Stuff**  |\n",
    "\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Agora vamos colocar em prática\n",
    "\n",
    "\n",
    "##### **1- Escolher o Modelo e Definir os Parâmetros**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crio a variável 'llm' que acessa o modelo de chat da OpenAI, usando a classe 'ChatOpenAI' do Langchain, que conecta com a API da OpenAI.\n",
    "llm = ChatOpenAI(model_name = \"gpt-4.1-mini\", temperature=0) # Defino o modelo da OpenAI a ser usado, e a temperatura, que controla o quão criativo o modelo deve ser. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**`O que é temperatura de um modelo?`**\n",
    "\n",
    "Temperatura é um parâmetro que controla o grau de aleatoriedade ou criatividade nas respostas geradas por modelos como o GPT. \n",
    "Essa temperatura varia de 0 a 1, sendo:\n",
    "- `0` respostas mais diretas, exatas e consistentes.\n",
    "- `1` respostas mais criativas, abertas e variáveis.\n",
    "\n",
    "O valor default da temperatura ao usar o ChatOpenAI é 0.7, que significa que o modelo não vai gerar respostas completamente aleatórias, mas também não serão totalmente fixas. \n",
    "\n",
    "\n",
    "**`ATENÇÃO`**: A temperatura deverá ser ajustada de acordo com o tipo de documento que foi carregado e o objetivo da aplicação. É recomendado que a temperatura para aplicações de RAG seja 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.1- LCEL: Langchain Expression Language**\n",
    "\n",
    "LCEL é uma linguagem declarativa e funcional criada dentro do Langchain, que significa que:\n",
    "- Declara o que quer que aconteça como: entrada -> prompt -> modelo -> saída \n",
    "- Segue princípios da programação funcional:\n",
    "    - Cada componente é uma função\n",
    "    - Encadea as funções com invoke()\n",
    "    - As funções não mudam o estado interno das outras, mas se compõe para formar funções maiores a partir de pequenas. \n",
    "\n",
    "Isso permite que você conecte os componentes de forma encadeada, reduzindo código repetitivo e tornando a pipeline reutilizável. \n",
    "\n",
    "##### **`Mas e na prática, como funciona?`**\n",
    "\n",
    "Na prática, o LCEL permite que você monte sua pipeline RAG como um **fluxo contínuo de execução**, conectando os blocos com o operador `|`.  \n",
    "Cada bloco é um componente que **transforma ou utiliza os dados** antes de passá-los para o próximo.\n",
    "\n",
    "1. Definir o template do prompt, chamando o retriever e a entrada do usuário\n",
    "2. Substituímos os placeholders no prompt com os valores reais, e gero o prompt completo\n",
    "3. Passo o prompt completo para o LLM, que gera uma resposta bruta.\n",
    "4. Converto a resposta bruta do LLM em uma string limpa pronta para ser exibida. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cria uma variável 'parser' que inicializa a classe 'StrOutputParser', que é usada para converter a resposta gerada pelo modelo em uma string limpa, removendo formatações desnecessárias e deixando o texto pronto para uso.\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Cria uma cadeia RAG completa, que conecta todos os componentes (retriever, prompt, llm e parser) para formar um fluxo funcional usando LCEL\n",
    "rag_chain = (\n",
    "    { # Define o formato de entrada esperado pela cadeia\n",
    "        \"context\": retriever, # Chama o retriever para buscar o contexto relevante\n",
    "        \"input\": RunnablePassthrough() # Usa o RunnablePassthrough para passar a entrada do usuário sem alterações\n",
    "    }\n",
    "    | prompt # O dicionário {input, context} é passado para o ChatpromptTemplate, substitui os placeholders {input} e {context} no prompt com os valores reais e gera o prompt completo para a LLM.\n",
    "    | llm # O LLM é chamado, recebe o prompt completo e gera uma resposta bruta em um formato não estruturado.\n",
    "    | parser # O parser pega a resposta bruta do modelo, e a converte em uma string limpa e estruturada, pronta para ser usada ou exibida.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **`Teste Funcional da Pipeline Completa`**\n",
    "\n",
    "Agora com todos os componentes conectados, é hora de simular o comportamento real da aplicação com uma pergunta de exemplo. \n",
    "Para isso, a cadeia é executada pelo método 'invoke' ao passar a pergunta \"What are the key issues related to ethics in Brazil?\" como string. A variável 'response' contém o resultado da cadeia, que é a resposta da LLM. \n",
    "A pergunta é relacionada ao documento carregado e deve ser adaptada conforme o documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Retrieval-Augmented Generation (RAG) method combines document retrieval with natural language generation by first retrieving relevant document chunks from an external knowledge base through semantic similarity calculations. This retrieval process is integrated with language generation models to enhance responses. Specifically, the RAG process involves indexing documents by splitting them into chunks, retrieving the most relevant chunks based on the input query, and then using these retrieved documents to augment the generation of natural language responses. This synergy between retrieval and generation improves the overall efficiency and quality of the system's outputs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = rag_chain.invoke(\"How does the RAG method combine document retrieval with natural language generation?\") # Chama a cadeia RAG com a pergunta do usuário\n",
    "print(response) # exibir a resposta final gerada pela cadeia RAG, que é uma string limpa e estruturada, pronta para ser usada ou exibida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.2 - Create Stuff: Funções Utilitárias**\n",
    "\n",
    "Por mais que já tenhamos gerado a resposta final da LLM em um formato estruturado no passo anterior, o método LCEL é uma forma manual e controlada de montar o fluxo da pipeline, em que você define cada etapa e pode modificar seu fluxo a gosto, ideal para customização. \n",
    "\n",
    "Porém, há outro jeito de conectar os componentes e gerar esse fluxo de um modo mais simplificado e automático, sem muita customização e controle, usando funções utilitárias como 'create_stuff_documents_chain' e 'create_retrieval_chain'. Vamos nos aprofundar nesse método agora:\n",
    "\n",
    "**`1. Primeiro usamos a função do langchain 'create_stuff_documents_chain' que cria uma \"document chain\", ou seja, uma cadeia responsável por gerar a resposta final da LLM com base em documentos de contexto. Internamente, a função:`**\n",
    "\n",
    "- Espera receber documentos (normalmente vindos do retriever)\n",
    "- Insere esses documentos no campo `{context}` do prompt\n",
    "- Insere a pergunta do usuário no campo `{input}`\n",
    "- Envia o prompt completo para o modelo de linguagem (LLM)\n",
    "- Retorna apenas a resposta final gerada pela LLM\n",
    "\n",
    "Pensa nela como a parte que monta o prompt final e gera a resposta.\n",
    "\n",
    "**`2. Depois usamos a função 'create_retrieval_chain' que conecta o retriever à cadeia de geração de resposta (criada acima), formando o fluxo completo da RAG. Internamente, a função:`**\n",
    "\n",
    "- Usa o retriever para buscar documentos relevantes com base na pergunta\n",
    "- Envia esses documentos para a `question_answer_chain`\n",
    "- Usa a LLM para gerar a resposta final\n",
    "- Retorna um dicionário contendo:\n",
    "  - \"answer\" → a resposta gerada pela LLM\n",
    "  - \"context\" → os documentos utilizados como suporte\n",
    "\n",
    "Pensa nela como a estrutura completa da RAG, que junta:\n",
    "- a busca por contexto (retriever)  \n",
    "- com a geração da resposta (LLM)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cria uma cadeia de documentos do tipo \"stuff\" (junção simples de documentos) usando o modelo de linguagem (LLM) e o prompt definido. Essa cadeia é responsável por gerar a resposta final da RAG com base no contexto recuperado.\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "#Cria a cadeia completa da RAG (Retrieval-Augmented Generation), conectando o retriever com a cadeia de geração de resposta (question_answer_chain) que foi definida acima.\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **`Teste Funcional da Pipeline Completa`**\n",
    "\n",
    "Agora com todos os componentes conectados, é hora de simular o comportamento real da aplicação com uma pergunta de exemplo. \n",
    "Para isso, a cadeia é executada pelo método 'invoke' ao passar a pergunta \"What are the key issues related to ethics in Brazil?\" como string. A variável 'res' contém o resultado da cadeia, que é a resposta da LLM. \n",
    "A pergunta é relacionada ao documento carregado e deve ser adaptada conforme o documento carregado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Retrieval-Augmented Generation (RAG) method combines document retrieval with natural language generation by first retrieving relevant document chunks from an external knowledge base through semantic similarity calculations. This retrieval step provides pertinent information that is then integrated into the language generation process. By combining \"Retrieval,\" \"Generation,\" and \"Augmentation,\" RAG enhances language models to produce more informed and accurate responses. The process typically involves indexing documents by splitting them into chunks, retrieving relevant chunks based on the query, and then generating responses that incorporate the retrieved information, thereby improving the overall efficiency and robustness of the system.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = rag_chain.invoke({\"input\": \"How does the RAG method combine document retrieval with natural language generation?\"}) # Chama a cadeia RAG com a pergunta do usuário\n",
    "res[\"answer\"] # exibir a resposta final gerada pela cadeia RAG, que é uma string limpa e estruturada, pronta para ser usada ou exibida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **`Vizualizando o Contexto`**\n",
    "\n",
    "Após executar a cadeia RAG e obter a resposta, também é possível inspecionar os documentos que foram recuperados como contexto e utilizados como base para a resposta final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
      "enhances LLMs by retrieving relevant document chunks from\n",
      "external knowledge base through semantic similarity calcu-\n",
      "of developing specialized strategies to integrate retrieval with\n",
      "language generation models, highlighting the need for further\n",
      "research and exploration into the robustness of RAG.\n",
      "3\n",
      "Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\n",
      "to the RAG process, specifically focusing on the aspects\n",
      "of “Retrieval”, “Generation” and “Augmentation”, and\n",
      "delve into their synergies, elucidating how these com-\n",
      "responses, thus improving the overall efficiency of the RAG\n",
      "system. To capture the logical relationship between document\n",
      "content and structure, KGP [91] proposed a method of building\n"
     ]
    }
   ],
   "source": [
    "# Percorre cada documento presente na lista de contexto retornada pela RAG\n",
    "for context in res[\"context\"]:\n",
    "    print(context.page_content) # Imprime o conteúdo textual (texto bruto) de cada documento recuperado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fontes:`\n",
    "\n",
    "- Construindo a RAG Chain com LCEL: https://towardsdatascience.com/building-a-rag-chain-using-langchain-expression-language-lcel-3688260cad05/\n",
    "- Create Stuff Documentation: https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Conclusão`**\n",
    "\n",
    "Com esta pipeline completa de RAG (Retrieval-Augmented Generation) usando Langchain e OpenAI, conseguimos:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carregar e preparar documentos em PDF como base de conhecimento.\n",
    "\n",
    "- Dividir o conteúdo em chunks otimizados para recuperação.\n",
    "\n",
    "- Gerar embeddings vetoriais e armazená-los em uma vector store.\n",
    "\n",
    "- Configurar um retriever para buscas semânticas.\n",
    "\n",
    "- Montar prompts estruturados e personalizados.\n",
    "\n",
    "- Criar fluxos de execução automatizados com LCEL e funções utilitárias.\n",
    "\n",
    "- Simular e testar o comportamento real da aplicação de forma modular e reutilizável.\n",
    "\n",
    "Esse fluxo é uma base robusta para qualquer aplicação que envolva respostas baseadas em documentos, como assistentes jurídicos, analisadores de contratos, suporte técnico inteligente, entre outros.\n",
    "\n",
    "\n",
    "**`Referências e Materiais Recomendados`**\n",
    "\n",
    "\n",
    "**Documentação Oficial**\n",
    "\n",
    "LangChain Documentation – documentação completa, com tutoriais e exemplos práticos. (https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "LangChain Prompt Guide – guia oficial para entender e construir bons prompts.(https://js.langchain.com/docs/how_to/graph_prompting/)\n",
    "\n",
    "OpenAI API Reference – referência da API usada para gerar embeddings e respostas. (https://platform.openai.com/docs/overview)\n",
    "\n",
    "**Artigos e Guias** \n",
    "\n",
    "LangChain: Getting Started (Towards Data Science) – uma introdução prática e bem explicada. (https://www.datacamp.com/tutorial/introduction-to-langchain-for-data-engineering-and-data-applications)\n",
    "\n",
    "Advanced Retriever Techniques – para explorar outros tipos de retrievers além do Naive Retriever. (https://towardsdatascience.com/advanced-retriever-techniques-to-improve-your-rags-1fac2b86dd61/)\n",
    "\n",
    "Vector Databases 101 (Pinecone) – excelente explicação sobre o papel das vector stores em sistemas de IA. (https://www.pinecone.io/learn/vector-database/)\n",
    "\n",
    "**Extras para se aprofundar**\n",
    "\n",
    "LangChain YouTube Channel – vídeos curtos e objetivos para entender o funcionamento das ferramentas. (https://www.youtube.com/@LangChain)\n",
    "\n",
    "Playlist Completa sobre RAG do Langchain. (https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
